# apiKey: "None"
# apiKeyRequired: false
# url: "http://localhost:11434/api/chat"
# #model: "hf.co/unsloth/Qwen3-8B-GGUF:Q4_K_S"
# #model: "hf.co/unsloth/Qwen3-8B-GGUF:Q3_K_S"
# #model: "vanilj/phi-4-unsloth"
# model: "qwen2.5:7b"

# apiKey: "None"
# apiKeyRequired: false
# url: "http://localhost:11434/api/chat"
# model: "hf.co/unsloth/Qwen3-8B-GGUF:Q4_K_S"

# apiKeyRequired: true
# url: "https://api.fireworks.ai/inference/v1/chat/completions"
# model: "accounts/fireworks/models/qwen3-8b"

apiKey: "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"
apiKeyRequired: true
url: "https://api.openai.com/v1/chat/completions"
model: "gpt-4o-mini"

modelParams:
  # # Qwen3 GGUF local
  # temperature: 0.2
  # max_tokens: 4096
  # top_p: 0.8
  # top_k: 20
  # min_p: 0.05
  # frequency_penalty: 0
  # presence_penalty: 0
  # num_ctx: 8192
  
  #Fire works
  # temperature: 0.2
  # top_p: 0.9
  # min_p: 0.05
  # frequency_penalty: 0
  # presence_penalty: 0

  # ChatGPT
  temperature: 0.2
  top_p: 0.9
  frequency_penalty: 0
  presence_penalty: 0

  # Qwen3 recommended
  # temperature: 0.7
  # max_tokens: 4096
  # top_p: 0.8
  # top_k: 20
  # min_p: 0
  # frequency_penalty: 0
  # presence_penalty: 0
  # num_ctx: 8192
retryCount: 1
batchSize: 3
skipLineValidation: true
correctionPromptsEnabled: false
translateFlagged: true